# Соревнование Titanic
## Оглавление
1. [Описание](#описание)
2. [Мотивация](#мотивация)
3. [Анализ данных](#анализ-данных)
4. [Подготовка данных](#подготовка-данных)
    1. [Обработка категориальных признаков](#обработка-категориальных-признаков)
    2. [Масштабирование данных](#масштабирование-данных)
    3. [Разделение на тренировочную и тестовую выборки](#разделение-на-тренировочную-и-тестовую-выборки)
5. [Выбор модели](#выбор-модели)
6. [Обучение модели](#обучение-модели)
7. [Оценка и результаты](#оценка-и-результаты)
8. [Заключение](#заключение)

## Описание
Соревнование "Titanic: Machine Learning from Disaster" на платформе Kaggle является одним из наиболее популярных и важных соревнований в области машинного обучения и анализа данных. Оно предоставляет участникам возможность применить свои навыки и знания в области машинного обучения для создания модели, способной предсказывать выживаемость пассажиров знаменитого корабля "Титаник".<br><br>
**Цель соревнования** заключается в создании модели, которая на основе данных о пассажирах (таких как возраст, пол, класс билета и т.д.) сможет предсказать, выживет ли пассажир или нет. В данном контексте "1" обозначает выживших, а "0" - погибших. Участники должны проанализировать и обработать предоставленные данные, выбрать подходящие признаки и алгоритмы, обучить модель на обучающем наборе данных и протестировать её на тестовой выборке.

## Мотивация:
Мой выбор участвовать в соревновании Titanic на Kaggle обусловлен несколькими ключевыми факторами. Прежде всего, Titanic - это не только один из самых известных датасетов в мире Data Science, но и некая историческая аналогия, которая сразу привлекает внимание. Однако за этой привлекательной историей скрывается глубокое погружение в анализ данных и применение машинного обучения.<br><br>
Выбрав Titanic в качестве первого соревнования, я осознанно решил начать с чего-то, что поможет мне освоить основы. Это соревнование предоставляет идеальную площадку для изучения базовых понятий машинного обучения, предобработки данных и создания первых моделей. Помимо этого, оно также предоставляет прекрасную возможность ознакомиться с различными методами анализа данных и стратегиями решения задачи классификации.

## Анализ данных:
Мой путь в этом соревновании начался с внимательного анализа предоставленных данных. После осмотра выборок train и test стало ясно, что передо мной стоит задача работы с пятью категориальными и семью количественными признаками. Это первый шаг, который помог мне в разработке стратегии для создания модели. 

```python
train.head()
```

```python
test.head()
```

```python
train.info()
```
Стоит отметить, что анализ показал наличие пропущенных значений в данных. Это выявление стало важным этапом, так как понимание, какие признаки и насколько подвержены пропущенным значениям, будет влиять на последующий этап обработки данных. Планирование способов заполнения пропусков стало ключевой задачей, которую я рассматриваю в рамках дальнейших шагов в данном соревновании.

Этот анализ данных не только помог мне оценить характеристики данных, но и сформировать начальное представление о том, какой путь мне следует пройти для успешного решения задачи предсказания выживаемости пассажиров на "Титанике".

## Подготовка данных:
Перед тем как приступить к созданию модели, я осознанно решил провести начальную фильтрацию и отбор столбцов в данных. Важным шагом было определить, какие столбцы предоставленных данных не будут иметь существенного вклада в обучение модели.

Так, после внимательного анализа, я принял решение исключить столбец с id пассажира, поскольку он не предоставляет полезной информации для обучения модели. Также в список исключенных столбцов вошли Name, Ticket и Cabin. Определение выживаемости пассажира вряд ли зависит от этих данных, поэтому их исключение способствовало сокращению размерности данных и улучшению обобщающей способности модели.

Такой подход позволил мне более осознанно подойти к формированию данных, исключить ненужную информацию и подготовиться к следующему этапу - обработке и предобработке данных для обучения модели.

## Обработке и предобработка данных для обучения модели:
### Обработка категориальных признаков
После того как я провёл предварительную обработку данных, настал момент решения, как эффективно работать с категориальными признаками. Одним из наиболее распространённых методов является OneHotEncoding, который позволяет перевести категориальные переменные в числовой формат, при этом избегая ненужной упорядоченности или искажения значений.

Для применения OneHotEncoding к данным я создал функцию FeatureEncoder, которая автоматизировала этот процесс. Применяя её к train выборке, я смог преобразовать категориальные признаки в бинарные столбцы, что позволило модели учитывать разные категории в независимости друг от друга.

```python
def FeatureEncoder(X):
    encoder = OneHotEncoder()
    matrix = encoder.fit_transform(X[['Embarked']]).toarray()

    column_names = ["C", "S", "Q", "N"]

    for i in range(len(matrix.T)):
        X[column_names[i]] = matrix.T[i]

    matrix = encoder.fit_transform(X[['Sex']]).toarray()  
    column_names = ["Female", "Male"]

    for i in range(len(matrix.T)):
        X[column_names[i]] = matrix.T[i]

    return X
```

С учетом сделанных исключений, я сформировал переменную X, включающую в себя dataframe без столбца Survived, который содержит правильные ответы. Вектор правильных ответов (Survived) я поместил в переменную y. Это предоставило мне четкое разделение между входными признаками и целевой переменной, что является фундаментом для дальнейшего построения и обучения модели.
```python
X = DropFeatures(train_data)
y = train['Survived']
```
### Масштабирование данных
Для повышения эффективности обучения модели я применил метод масштабирования данных. Это важный этап предобработки, который позволяет привести все признаки к общему масштабу и диапазону значений. 

Я использовал стандартизацию (StandardScaler) из библиотеки `scikit-learn`, которая масштабирует признаки так, чтобы среднее значение стало равным 0, а стандартное отклонение - 1.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
y_scaled = y.to_numpy()
```
### Разделение на тренировочную и тестовую выборки
После всех этапов предобработки данных, следующим шагом было разделение данных на тренировочную и тестовую выборки. Это позволяет оценить качество обучения модели на отложенных данных и проверить, насколько успешно модель будет работать на новых наборах данных.

Для этого я использовал функцию `train_test_split` из библиотеки `scikit-learn`, которая случайным образом разбивает данные на две части: одну для обучения модели (тренировочную выборку) и другую для оценки её производительности (тестовую выборку).

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=.3, random_state=17) 
```

## Выбор модели:
Из всех моделей я решил выбрать `RandomForestClassifier`. Мой выбор обусловлен несколькими факторами:

- Во-первых, случайный лес является ансамблевым методом, который объединяет множество деревьев решений для улучшения обобщающей способности модели. Это позволяет избежать переобучения и учесть разнообразие признаков.
- Во-вторых, случайный лес хорошо справляется с задачами классификации, так как способен обрабатывать как количественные, так и категориальные признаки. Учитывая многообразие данных в задаче предсказания выживаемости, я считаю, что этот метод может дать хорошие результаты.
 

