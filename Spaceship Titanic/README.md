# Соревнование Spaceship Titanic
## Оглавление
1. [Описание](#описание)
2. [Визуализация](#визуализация)
3. [Анализ данных](#анализ-данных)
4. [Подготовка данных](#подготовка-данных)
5. [Выбор модели](#выбор-модели)
6. [Обучение модели](#обучение-модели)
7. [Предсказание](#предсказание)
8. [Оценка и результаты](#оценка-и-результаты)
9. [Заключение](#заключение)

## Описание
В 2912 году корабль "Spaceship Titanic" столкнулся с пространственно-временной аномалией, и половина пассажиров была перемещена в альтернативное измерение. Вам предстоит по записям из поврежденной компьютерной системы определить, какие пассажиры были затронуты аномалией.

## Анализ данных:
Мой путь в этом соревновании начался с внимательного анализа предоставленных данных. После осмотра выборок train и test стало ясно, что передо мной стоит задача работы с пятью категориальными и семью количественными признаками. Это первый шаг, который помог мне в разработке стратегии для создания модели. 

```python
train_data.head()
```

```python
train_data.info()
```

```python
train_data.describe()
```
```python
test_data.head()
```

Стоит отметить, что анализ показал наличие пропущенных значений в данных. Это выявление стало важным этапом, так как понимание, какие признаки и насколько подвержены пропущенным значениям, будет влиять на последующий этап обработки данных. 

## Визуализация:
Прежде чем приступать к подготовке данных, я решил визуализировать имеющуюся информацию, чтобы понять с чем мне предстоит работать.

### График распределения возрата:
<img src="[https://github.com/Lapamore/Kaggle_competitions/blob/main/Spaceship%20Titanic/img/hist.png]" alt="График распределения возрата" width="300" height="200">
Заметно, что основная часть пассажиров космического корабля находится в возрастной группе от 13 до 38 лет.

### График Boxplot
![График распределения возрата](https://github.com/Lapamore/Kaggle_competitions/blob/main/Spaceship%20Titanic/img/boxplot.png)
Анализ графика Boxplot позволяет выявить наличие выбросов в данных столбца "Age". Это нюанс, с которым мы учтем при последующей обработке.

### График корреляции:
![График распределения возрата](https://github.com/Lapamore/Kaggle_competitions/blob/main/Spaceship%20Titanic/img/corr.png)
Изучая матрицу корреляции, можно выделить переменные, между которыми наблюдается высокая степень взаимосвязи.

## Подготовка данных:
Перед тем как приступить к созданию модели, я осознанно решил провести начальную фильтрацию и отбор столбцов в данных. Важным шагом было определить, какие столбцы предоставленных данных не будут иметь существенного вклада в обучение модели.

Так, после внимательного анализа, я принял решение исключить столбец с id пассажира, поскольку он не предоставляет полезной информации для обучения модели. Также в список исключенных столбцов вошли Name, Ticket и Cabin. Определение выживаемости пассажира вряд ли зависит от этих данных, поэтому их исключение способствовало сокращению размерности данных и улучшению обобщающей способности модели.

Такой подход позволил мне более осознанно подойти к формированию данных, исключить ненужную информацию и подготовиться к следующему этапу - обработке и предобработке данных для обучения модели.

## Обработке и предобработка данных для обучения модели:
### Обработка категориальных признаков
После того как я провёл предварительную обработку данных, настал момент решения, как эффективно работать с категориальными признаками. Одним из наиболее распространённых методов является OneHotEncoding, который позволяет перевести категориальные переменные в числовой формат, при этом избегая ненужной упорядоченности или искажения значений.

Для применения OneHotEncoding к данным я создал функцию FeatureEncoder, которая автоматизировала этот процесс. Применяя её к train выборке, я смог преобразовать категориальные признаки в бинарные столбцы, что позволило модели учитывать разные категории в независимости друг от друга.

```python
def FeatureEncoder(X):
    encoder = OneHotEncoder()
    matrix = encoder.fit_transform(X[['Embarked']]).toarray()

    column_names = ["C", "S", "Q", "N"]

    for i in range(len(matrix.T)):
        X[column_names[i]] = matrix.T[i]

    matrix = encoder.fit_transform(X[['Sex']]).toarray()  
    column_names = ["Female", "Male"]

    for i in range(len(matrix.T)):
        X[column_names[i]] = matrix.T[i]

    return X
```

С учетом сделанных исключений, я сформировал переменную X, включающую в себя dataframe без столбца Survived, который содержит правильные ответы. Вектор правильных ответов (Survived) я поместил в переменную y. Это предоставило мне четкое разделение между входными признаками и целевой переменной, что является фундаментом для дальнейшего построения и обучения модели.
```python
X = DropFeatures(train_data)
y = train['Survived']
```
### Масштабирование данных
Для повышения эффективности обучения модели я применил метод масштабирования данных. Это важный этап предобработки, который позволяет привести все признаки к общему масштабу и диапазону значений. 

Я использовал стандартизацию (StandardScaler) из библиотеки `scikit-learn`, которая масштабирует признаки так, чтобы среднее значение стало равным 0, а стандартное отклонение - 1.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
y_scaled = y.to_numpy()
```
### Разделение на тренировочную и тестовую выборки
После всех этапов предобработки данных, следующим шагом было разделение данных на тренировочную и тестовую выборки. Это позволяет оценить качество обучения модели на отложенных данных и проверить, насколько успешно модель будет работать на новых наборах данных.

Для этого я использовал функцию `train_test_split` из библиотеки `scikit-learn`, которая случайным образом разбивает данные на две части: одну для обучения модели (тренировочную выборку) и другую для оценки её производительности (тестовую выборку).

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=.3, random_state=17) 
```

## Выбор модели:
Из всех моделей я решил выбрать `RandomForestClassifier`. Мой выбор обусловлен несколькими факторами:

- Во-первых, случайный лес является ансамблевым методом, который объединяет множество деревьев решений для улучшения обобщающей способности модели. Это позволяет избежать переобучения и учесть разнообразие признаков.
- Во-вторых, случайный лес хорошо справляется с задачами классификации, так как способен обрабатывать как количественные, так и категориальные признаки. Учитывая многообразие данных в задаче предсказания выживаемости, я считаю, что этот метод может дать хорошие результаты.
 
## Обучение модели
Для настройки параметров модели и достижения наилучшей производительности, я применил метод `GridSearchCV`. Этот метод позволяет обучать модель на кросс-валидации, автоматически перебирая различные комбинации заданных параметров.

В результате, переменная с наилучшими параметрами модели будет создана автоматически. Это позволяет упростить и ускорить процесс поиска наиболее оптимальных параметров для данной выборки данных.

```python
clf = RandomForestClassifier(random_state=17)

params = {
    'n_estimators':[10, 100, 200, 500, 600],
    'max_depth':[None, 5, 10],
    "min_samples_split": [2, 3 ,4],
}

search = GridSearchCV(clf, params, cv=3, scoring='accuracy')
search.fit(X_train, y_train)

best_model = search.best_estimator_
best_model.score(X_test, y_test)
```
Accuracy на X_test и y_test = 0.802

## Предсказание
Пройдя через все этапы обработки и настройки модели, я перешел к фазе предсказания. Для этого использовал обученную модель с наилучшими параметрами.

```python
pred = best_model.predict(X_data_final_test)
```

После выполнения предсказаний, я создал dataframe, где каждому человеку предсказано, выживет ли он или нет. Это позволило оценить, какие пассажиры, возможно, имеют больший шанс выжить на основе разработанной модели.

```python
df_pred = pd.DataFrame(test['PassengerId'])
df_pred['Survived'] = pred 
df_pred.to_csv("predictions/Titanic prediction.csv", index=False)
```

## Оценка и результаты
Следующим этапом работы стала загрузка `Titanic prediction.csv` на платформу Kaggle, где я оценил точность своей модели предсказания на тестовой выборке.

![Accuracy](https://github.com/Lapamore/Kaggle_competitions/blob/main/Titanic/img/score_2.png?raw=true)

После загрузки предсказаний на платформу Kaggle, я получил оценку точности моей модели на основе метрики, предоставленной в задаче. Это позволило мне понять, насколько успешно моя модель обобщает данные и делает предсказания на новых данных.

![Место в таблице](https://github.com/Lapamore/Kaggle_competitions/blob/main/Titanic/img/score.png?raw=true)

Этот этап закрывает мой проект и дает понимание того, насколько успешно моя модель справляется с задачей предсказания выживаемости на "Титанике".

## Заключение
На этом этапе я заканчиваю свое описание решения поставленной задачи. Хотелось бы подчеркнуть, что именно это соревнование помогло мне разобраться со многими недопониманиями. Надеюсь, что Вы оцените мое решение. Спасибо!
